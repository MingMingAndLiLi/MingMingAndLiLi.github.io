---
layout: single
permalink: /publication/2022-06-22-hsc4d/
title: "HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR"
date: 2022-06-14
author_profile: false
collection: publications
venue: CVPR
journal: IEEE Conference on Computer Vision and Pattern Recognition
author: "<strong>Yudi Dai<sup>1</sup></strong>, Yitai Lin<sup>1</sup>, Chenglu Wen<sup>1, *</sup>, Siqi Shen<sup>1</sup>, Lan Xu<sup>2</sup>, Jingyi Yu<sup>2</sup>, Yuexin Ma<sup>2</sup>, Cheng Wang<sup>1</sup>"
paper: "https://arxiv.org/abs/2203.09215"
project: "/publication/2022-06-22-hsc4d"
code: "https://github.com/climbingdaily/HSC4D"
videopath: "/images/HSC4D.mp4"

header:
  overlay_color: "#9EA0A3"
  # overlay_filter: rgba(0, 0, 0, 0.3)
  # overlay_image: /images/overview.png
  image_description: "A description of the image"
  actions:
    - label: "Arxiv"
      url: "https://arxiv.org/abs/2203.09215"
    - label: "CODE"
      url: "https://github.com/climbingdaily/HSC4D"
  address: "$^1$spAital Sensing and Computing Lab, School of Informatics, Xiamen Universtiy, China<br>
  $^2$Shanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech Universtiy, China"


redirect_from: 
  - "/publication/2022-06-22-hsc4d.html"
  - "/publication/hsc4d.html"
  - "/publication/hsc4d/"
  - "/hsc4d/"
  - "/hsc4d.html"
---

<img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/images/overview.png">
<div style="color:orange; border-bottom: 0px solid #d9d9d9;
display: inline-block;
color: #999;
padding: -2px;">3D human motions with accurate localization in diverse, challenging scenes. The top line shows a person walking from indoor to outdoor. The bottom row figures show the challenge cases, where the bottom right figure is the rock climbing's third-view from the camera. </div>


## Abstract
We propose Human-centered 4D Scene Capture (HSC4D) to accurately and efficiently create a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, and rich interactions between humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is space-free without any external devices' constraints and map-free without pre-built maps. Considering that IMUs can capture human poses but always drift for long-period use, while LiDAR is stable for global localization but rough for local positions and orientations, HSC4D makes both sensors complement each other by a joint optimization and achieves promising results for long-term capture. Relationships between humans and environments are also explored to make their interaction more realistic. To facilitate many down-stream tasks, like AR, VR, robots, autonomous driving, etc., we propose a dataset containing three large scenes (1k-5k $m^2$) with accurate dynamic human motions and locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.) and challenging human activities (exercising, walking up/down stairs, climbing, etc.) demonstrate the effectiveness and the generalization ability of HSC4D. The dataset and code is available at this [https URL](https://github.com/climbingdaily/HSC4D).

## Pipeline
<video 
width="100%"
style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="/images/hsc4d_pipeline.mp4" 
frameborder="no" 
allowfullscreen="false"
preload="" 
muted="muted" 
autoplay="autoplay"
playsinline="">
</video>
<div style="color:orange; border-bottom: 0px solid #d9d9d9;
display: inline-block;
color: #999;
padding: -2px;">The inputs are point cloud sequence and IMUs data. IMUs pose estimation and LiDAR mapping are performed, respectively. Synchronization and calibration are applied to prepare data for further optimization. Finally, graph-based optimization and joint optimization are performed to produce the global motion in the scene map. </div>

## Capturring system
<video 
width="100%" 
style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="/images/hsc4d_system.mp4" 
frameborder="no" 
allowfullscreen="false"
preload="" 
muted="muted" 
autoplay="autoplay"
playsinline="">
</video>
<div style="color:orange; border-bottom: 0px solid #d9d9d9;
display: inline-block;
color: #999;
padding: -2px;">Our lightweight wearable capturing system includes 17 body-attached IMUs and a hip-mounted LiDAR. LiDAR packages are connected to a DJI Manifold2-C mini-computer. We tilt up the LiDAR for 30° to get a good vertical scanning view. </div>

## Video
<iframe 
width="800" 
height="450" 
style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="https://www.youtube.com/embed/IY9FikM__i8" title="YouTube video player" 
frameborder="no" 
allowfullscreen="true"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture">
</iframe>

## Dataset and code
The detail about the code and dataset is available at my [Github](https://github.com/climbingdaily/HSC4D).
More information will be updated here later.

### Data structure
```
Dataset
├── campus_road
|  ├── campus_road.bvh
|  ├── campus_road_pos.csv
|  ├── campus_road_rot.csv
|  ├── campus_road.pcap
|  └── campus_road_lidar_trajectory.txt 
├── scenes
   ├── campus_road.pcd
   ├── campus_road_ground.pcd
```


### Data preprocess


### Data fusion


### Data optimization



## Copyright
The HSC4D dataset is published under the [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License](https://creativecommons.org/licenses/by-nc-sa/3.0/).You must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license. Contact us if you are interested in commercial usage.


## Citation
```
@misc{dai2022hsc4d,
    title={HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR},
    author={Yudi Dai and Yitai Lin and Chenglu Wen and Siqi Shen and Lan Xu and Jingyi Yu and Yuexin Ma and Cheng Wang},
    year={2022},
    eprint={2203.09215},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```
